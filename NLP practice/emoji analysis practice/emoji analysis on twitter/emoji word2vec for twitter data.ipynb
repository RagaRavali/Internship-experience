{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import heapq\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth  = 0\n",
    "path = 'E:/intern/aaaaaaaaaaaa/results.csv'\n",
    "df = pd.read_csv(path,encoding = \"ISO-8859-1\")\n",
    "\n",
    "def preprocess(tweet):\n",
    "    #tweet = tweet.lower()\n",
    "    exclude = {'/','\"', '&', '-', '#', '(', ')', '$', '@', '%', '!', '}', '[', ':', ']', '_', '|', ';', '{', '?', '=', '\\\\', '~', '*', ',', '^', '`'}\n",
    "    tweet = ''.join([i for i in tweet if i not in exclude])\n",
    "    tweet = \" \".join(tweet.split('#'))\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https://[^\\s]+))',' ',tweet)\n",
    "    tweet = re.sub(\"http\\S+\", \" \", tweet)\n",
    "    tweet = re.sub(\"https\\S+\", \" \", tweet)\n",
    "    tweet = re.sub(\"<ed>\",\" \",tweet)\n",
    "    tweet = re.sub(\"@\\S+\",\" \",tweet)\n",
    "    return tweet\n",
    "\n",
    "tweets = [preprocess(x) for x in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing all the frequencies of the emoticons used\n",
      "\n",
      "{'<U+00A0>': 166, '<U+00BD>': 108, '<U+00B8>': 54, '<U+0098>': 9, '<U+00B0>': 3, '<U+00BE>': 20, '<U+00BC>': 80, '<U+0084>': 10, '<U+008D>': 10, '<U+2764>': 13, '<U+FE0F>': 28, '<U+00B9>': 7, '<U+0080>': 6, '<U+00B1>': 14, '<U+008F>': 6, '<U+00B2>': 30, '<U+0095>': 7, '<U+2744>': 4, '<U+00A5>': 1, '<U+00B7>': 13, '<U+00AA>': 3, '<U+00B5>': 3, '<U+0099>': 4, '<U+00A9>': 3, '<U+2615>': 8, '<U+0096>': 2, '<U+00A8>': 3, '<U+00BB>': 6, '<U+2600>': 11, '<U+00B6>': 2, '<U+0097>': 6, '<U+0093>': 3, '<U+00AD>': 2, '<U+2728>': 2, '<U+0086>': 1, '<U+2705>': 3, '<U+008C>': 3, '<U+009F>': 1, '<U+00A7>': 5, '<U+00BF>': 9, '<U+008A>': 6, '<U+0083>': 1, '<U+008E>': 5, '<U+00B3>': 1, '<U+0089>': 1, '<U+2B50>': 1, '<U+009B>': 3, '<U+009E>': 5, '<U+00B4>': 5, '<U+00A3>': 2, '<U+0081>': 2, '<U+00AC>': 2, '<U+00BA>': 5, '<U+0082>': 10, '<U+26C4>': 2, '<U+0085>': 1, '<U+270B>': 1, '<U+23F0>': 1, '<U+263A>': 2, '<U+2661>': 1, '<U+30C4>': 1, '<U+009D>': 1, '<U+009A>': 1, '<U+008B>': 2, '<U+2604>': 1, '<U+0088>': 1, '<U+00AF>': 3, '<U+0087>': 1}\n",
      "\n",
      "Printing all the top 10 emoticons used in the tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<U+00A0>',\n",
       " '<U+00BD>',\n",
       " '<U+00BC>',\n",
       " '<U+00B8>',\n",
       " '<U+00B2>',\n",
       " '<U+FE0F>',\n",
       " '<U+00BE>',\n",
       " '<U+00B1>',\n",
       " '<U+2764>',\n",
       " '<U+00B7>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis = []\n",
    "\n",
    "emojis = [re.findall('\\<.*?\\>',s) for s in df.text]\n",
    "#print(emojis)\n",
    "\n",
    "emoji_list = list(itertools.chain.from_iterable(emojis))\n",
    "#print(emoji_list.value_counts()[:25])\n",
    "#print(set(emoji_list))\n",
    "emoji_freq = {}\n",
    "for e in emoji_list:\n",
    "    emoji_freq[e] = emoji_list.count(e)\n",
    "del emoji_freq['<ed>']\n",
    "print(\"Printing all the frequencies of the emoticons used\")\n",
    "print()\n",
    "print(emoji_freq) \n",
    "print()\n",
    "print(\"Printing all the top 10 emoticons used in the tweets\")   \n",
    "heapq.nlargest(10, emoji_freq, key=emoji_freq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-20 11:00:06,150 : INFO : collecting all words and their counts\n",
      "2017-06-20 11:00:06,154 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-20 11:00:06,158 : INFO : collected 1252 word types from a corpus of 3088 raw words and 247 sentences\n",
      "2017-06-20 11:00:06,162 : INFO : Loading a fresh vocabulary\n",
      "2017-06-20 11:00:06,170 : INFO : min_count=5 retains 76 unique words (6% of original 1252, drops 1176)\n",
      "2017-06-20 11:00:06,174 : INFO : min_count=5 leaves 1625 word corpus (52% of original 3088, drops 1463)\n",
      "2017-06-20 11:00:06,178 : INFO : deleting the raw counts dictionary of 1252 items\n",
      "2017-06-20 11:00:06,186 : INFO : sample=0.001 downsamples 76 most-common words\n",
      "2017-06-20 11:00:06,190 : INFO : downsampling leaves estimated 514 word corpus (31.6% of prior 1625)\n",
      "2017-06-20 11:00:06,194 : INFO : estimated required memory for 76 words and 400 dimensions: 281200 bytes\n",
      "2017-06-20 11:00:06,202 : INFO : resetting layer weights\n",
      "2017-06-20 11:00:06,210 : INFO : training model with 4 workers on 76 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-06-20 11:00:06,214 : INFO : expecting 247 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-06-20 11:00:06,242 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-06-20 11:00:06,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-20 11:00:06,250 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-20 11:00:06,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-20 11:00:06,258 : INFO : training on 15440 raw words (2515 effective words) took 0.0s, 118291 effective words/s\n",
      "2017-06-20 11:00:06,262 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-06-20 11:00:06,266 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "wt = [list(x.split()) for x in tweets]\n",
    "\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 400    # Word vector dimensionality  \n",
    "min_word_count = 5   # Minimum word count  \n",
    "num_workers = 4       # Number of threads to run in parallel  \n",
    "context = 10          # Context window size  \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec  \n",
    "print(\"Training model...\")  \n",
    "model = word2vec.Word2Vec(wt, workers=num_workers,   \n",
    "            size=num_features, min_count = min_word_count, \n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "print(model.init_sims(replace=True))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-20 11:04:58,050 : INFO : collecting all words and their counts\n",
      "2017-06-20 11:04:58,054 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-20 11:04:58,062 : INFO : collected 1252 word types from a corpus of 3088 raw words and 247 sentences\n",
      "2017-06-20 11:04:58,066 : INFO : Loading a fresh vocabulary\n",
      "2017-06-20 11:04:58,074 : INFO : min_count=2 retains 278 unique words (22% of original 1252, drops 974)\n",
      "2017-06-20 11:04:58,082 : INFO : min_count=2 leaves 2114 word corpus (68% of original 3088, drops 974)\n",
      "2017-06-20 11:04:58,090 : INFO : deleting the raw counts dictionary of 1252 items\n",
      "2017-06-20 11:04:58,094 : INFO : sample=0.001 downsamples 69 most-common words\n",
      "2017-06-20 11:04:58,098 : INFO : downsampling leaves estimated 1092 word corpus (51.7% of prior 2114)\n",
      "2017-06-20 11:04:58,102 : INFO : estimated required memory for 278 words and 40 dimensions: 227960 bytes\n",
      "2017-06-20 11:04:58,110 : INFO : resetting layer weights\n",
      "2017-06-20 11:04:58,130 : INFO : training model with 3 workers on 278 vocabulary and 40 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-06-20 11:04:58,134 : INFO : expecting 247 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-06-20 11:04:58,162 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-20 11:04:58,174 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-20 11:04:58,178 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-20 11:04:58,186 : INFO : training on 15440 raw words (5398 effective words) took 0.0s, 147816 effective words/s\n",
      "2017-06-20 11:04:58,190 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-06-20 11:04:58,194 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Thank', 0.812110424041748),\n",
       " ('hun', 0.7699514031410217),\n",
       " ('amp', 0.7519063949584961),\n",
       " ('vote', 0.7466974258422852),\n",
       " ('well', 0.7459235191345215),\n",
       " ('a', 0.7458176612854004),\n",
       " ('<U+00A0><U+00BD>', 0.7337977886199951),\n",
       " ('Wednesday', 0.7331104278564453),\n",
       " ('morning', 0.7276653051376343),\n",
       " ('time', 0.7257640361785889),\n",
       " ('have', 0.7236992120742798),\n",
       " ('day', 0.7155070900917053),\n",
       " ('I', 0.7099223136901855),\n",
       " ('very', 0.7073180675506592),\n",
       " ('the', 0.7013233304023743)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [list(x.split()) for x in tweets]\n",
    "#print(words)\n",
    "\n",
    "num_features = 40    # Word vector dimensionality  \n",
    "min_word_count = 2   # Minimum word count  \n",
    "num_workers = 3      # Number of threads to run in parallel  \n",
    "context = 5         # Context window size  \n",
    "downsampling = 1e-3 \n",
    "model = word2vec.Word2Vec(words, workers=num_workers,size=num_features, min_count = min_word_count,window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model.most_similar('good',topn =15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
